{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURO-EVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleAgent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CartPoleAgent, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 124)\n",
    "        # self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(124, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initialized_agents(folderName):\n",
    "    agents = []\n",
    "    for path in os.listdir(folderName):\n",
    "        if path[-4:] == '.pth':\n",
    "            try:\n",
    "                model = CartPoleAgent()\n",
    "                model.load_state_dict(torch.load(folderName + '/' + path))\n",
    "                agents.append(model)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return agents\n",
    "\n",
    "    get_initialized_agents(folderName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents(agents):\n",
    "    game_actions = 2\n",
    "    reward_agents = []\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    env.spec.reward_threshold = 500\n",
    "    \n",
    "    for agent in agents:\n",
    "        agent.eval()\n",
    "    \n",
    "        observation = env.reset()\n",
    "        \n",
    "        r, s = 0, 0\n",
    "        for _ in range(250):\n",
    "            \n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            r = r + reward\n",
    "            \n",
    "            s = s + 1\n",
    "            observation = new_observation\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        reward_agents.append(r)        \n",
    "        # reward_agents.append(s)\n",
    "    \n",
    "    return reward_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_average_score(agent, runs):\n",
    "    score = 0.\n",
    "    for i in range(runs):\n",
    "        score += run_agents([agent])[0]\n",
    "    return score / runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents_n_times(agents, runs):\n",
    "    return [return_average_score(agent, runs) for agent in agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(agent):\n",
    "    child_agent = copy.deepcopy(agent)\n",
    "    mutation_power = 0.02 # Set from https://arxiv.org/pdf/1712.06567.pdf\n",
    "    for param in child_agent.parameters():\n",
    "        if len(param.shape) == 4: # Weights of Conv2D\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    for i2 in range(param.shape[2]):\n",
    "                        for i3 in range(param.shape[3]):\n",
    "                            param[i0][i1][i2][i3] += mutation_power * np.random.randn()\n",
    "        \n",
    "        elif len(param.shape) == 2: # Weights of linear layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    param[i0][i1] += mutation_power * np.random.randn()\n",
    "        \n",
    "        elif len(param.shape) == 1: # Biases of linear layer or conv layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                \n",
    "                param[i0] += mutation_power * np.random.randn()\n",
    "\n",
    "    return child_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_children(agents, sorted_parent_indexes, elite_index):\n",
    "    children_agents = []\n",
    "    \n",
    "    for i in range(len(agents)-1):\n",
    "        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n",
    "        children_agents.append(mutate(agents[selected_agent_index]))\n",
    "\n",
    "    elite_child = add_elite(agents, sorted_parent_indexes, elite_index)\n",
    "    children_agents.append(elite_child)\n",
    "    elite_index = len(children_agents) - 1\n",
    "    \n",
    "    return children_agents, elite_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elite(agents, sorted_parent_indexes, elite_index=None, only_consider_top_n=10):\n",
    "    candidate_elite_index = sorted_parent_indexes[:only_consider_top_n]\n",
    "    \n",
    "    if elite_index is not None:\n",
    "        candidate_elite_index = np.append(candidate_elite_index,[elite_index])\n",
    "        \n",
    "    top_score = None\n",
    "    top_elite_index = None\n",
    "    \n",
    "    for i in candidate_elite_index:\n",
    "        score = return_average_score(agents[i],runs=5)\n",
    "        print(\"Score for elite i \", i, \" is \", score)\n",
    "        \n",
    "        if(top_score is None):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "        elif(score > top_score):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "            \n",
    "    print(\"Elite selected with index \",top_elite_index, \" and score\", top_score)\n",
    "    \n",
    "    child_agent = copy.deepcopy(agents[top_elite_index])\n",
    "    return child_agent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_agent(agent):\n",
    "    try:\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        \n",
    "        env_record = Monitor(env, './video', force=True)\n",
    "        observation = env_record.reset()\n",
    "        last_observation = observation\n",
    "        \n",
    "        r = 0\n",
    "        for _ in range(250):\n",
    "            env_record.render()\n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            new_observation, reward, done, info = env_record.step(action)\n",
    "            r=r+reward\n",
    "            observation = new_observation\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        env_record.close()\n",
    "        print(\"Rewards: \", r)\n",
    "\n",
    "    except Exception as e:\n",
    "        env_record.close()\n",
    "        print(e.__doc__)\n",
    "        print(e.message)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEVOModel(folderName, fileName):\n",
    "    game_actions = 2\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    agents = get_initialized_agents('./' + folderName)\n",
    "\n",
    "    top_limit = 5 # Number of top agents to consider as parents\n",
    "    generations = 10\n",
    "\n",
    "    elite_index = None\n",
    "    for generation in range(generations):\n",
    "        rewards = run_agents_n_times(agents, 10) # Average of k runs\n",
    "\n",
    "        sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit]\n",
    "        print('\\n')\n",
    "\n",
    "        top_rewards = []\n",
    "        for best_parent in sorted_parent_indexes:\n",
    "            top_rewards.append(rewards[best_parent])\n",
    "\n",
    "        print(\"Generation \", generation, \" | Mean rewards: \", np.mean(rewards), \" | Mean of top 5: \",np.mean(top_rewards[:5]))\n",
    "        # print(rewards)\n",
    "        print(\"Top \",top_limit,\" scores\", sorted_parent_indexes)\n",
    "        print(\"Rewards for top: \",top_rewards)\n",
    "\n",
    "        children_agents, elite_index = return_children(agents, sorted_parent_indexes, elite_index)\n",
    "        agents = children_agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        self.affine1 = nn.Linear(self.state_space, 124)\n",
    "        #self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(124, self.action_space)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "def sim_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    actions = policy(state)\n",
    "    _, action = actions.max(1)\n",
    "    return action.item()\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "def sim():\n",
    "    tot_reward = 0\n",
    "    state = env.reset()\n",
    "    for t in range(1, 10000):\n",
    "        action = sim_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        tot_reward += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "            print(tot_reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDQNmodel():\n",
    "    running_reward = 0\n",
    "    for i_episode in range(numEpisodes):\n",
    "        state, ep_reward = env.reset(), 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                #duration.append(t)\n",
    "                break\n",
    "\n",
    "        running_reward = 0.04 * ep_reward + (1 - 0.04) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            print(\"{},{}\".format(i_episode, ep_reward))\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95 # discount factor\n",
    "seed  = 543\n",
    "render = False\n",
    "log_interval = 10\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.spec.reward_threshold = 200\n",
    "numEpisodes = 15\n",
    "numSuccessions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1.00e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast reward: 33.00\tAverage reward: 1.32\n",
      "Episode 10\tLast reward: 18.00\tAverage reward: 7.22\n",
      "Episode 0\tLast reward: 33.00\tAverage reward: 1.32\n",
      "Episode 10\tLast reward: 129.00\tAverage reward: 24.93\n",
      "Episode 0\tLast reward: 42.00\tAverage reward: 1.68\n",
      "Episode 10\tLast reward: 53.00\tAverage reward: 21.60\n",
      "Episode 0\tLast reward: 70.00\tAverage reward: 2.80\n",
      "Episode 10\tLast reward: 57.00\tAverage reward: 27.05\n",
      "Episode 0\tLast reward: 36.00\tAverage reward: 1.44\n",
      "Episode 10\tLast reward: 385.00\tAverage reward: 51.52\n",
      "Episode 0\tLast reward: 500.00\tAverage reward: 20.00\n",
      "Episode 10\tLast reward: 130.00\tAverage reward: 112.78\n",
      "Episode 0\tLast reward: 109.00\tAverage reward: 4.36\n",
      "Episode 10\tLast reward: 116.00\tAverage reward: 43.53\n",
      "Episode 0\tLast reward: 121.00\tAverage reward: 4.84\n",
      "Episode 10\tLast reward: 71.00\tAverage reward: 32.78\n",
      "Episode 0\tLast reward: 62.00\tAverage reward: 2.48\n",
      "Episode 10\tLast reward: 105.00\tAverage reward: 32.32\n",
      "Episode 0\tLast reward: 123.00\tAverage reward: 4.92\n",
      "Episode 10\tLast reward: 260.00\tAverage reward: 66.75\n",
      "Episode 0\tLast reward: 286.00\tAverage reward: 11.44\n",
      "Episode 10\tLast reward: 500.00\tAverage reward: 138.46\n",
      "Episode 0\tLast reward: 263.00\tAverage reward: 10.52\n",
      "Episode 10\tLast reward: 238.00\tAverage reward: 96.19\n",
      "Episode 0\tLast reward: 248.00\tAverage reward: 9.92\n",
      "Episode 10\tLast reward: 436.00\tAverage reward: 64.31\n",
      "Episode 0\tLast reward: 258.00\tAverage reward: 10.32\n",
      "Episode 10\tLast reward: 276.00\tAverage reward: 67.56\n",
      "Episode 0\tLast reward: 284.00\tAverage reward: 11.36\n",
      "Episode 10\tLast reward: 219.00\tAverage reward: 73.71\n",
      "\n",
      "\n",
      "Generation  0  | Mean rewards:  147.0  | Mean of top 5:  240.56\n",
      "Top  5  scores [ 6 10 13  7 12]\n",
      "Rewards for top:  [250.0, 242.2, 239.9, 238.1, 232.6]\n",
      "Score for elite i  6  is  250.0\n",
      "Score for elite i  10  is  247.0\n",
      "Score for elite i  13  is  250.0\n",
      "Score for elite i  7  is  245.2\n",
      "Score for elite i  12  is  213.2\n",
      "Elite selected with index  6  and score 250.0\n",
      "\n",
      "\n",
      "Generation  1  | Mean rewards:  235.26666666666674  | Mean of top 5:  245.7\n",
      "Top  5  scores [ 2 14  9  5  6]\n",
      "Rewards for top:  [247.5, 247.4, 245.9, 244.4, 243.3]\n",
      "Score for elite i  2  is  243.0\n",
      "Score for elite i  14  is  250.0\n",
      "Score for elite i  9  is  244.6\n",
      "Score for elite i  5  is  250.0\n",
      "Score for elite i  6  is  242.6\n",
      "Score for elite i  14  is  249.6\n",
      "Elite selected with index  14  and score 250.0\n",
      "\n",
      "\n",
      "Generation  2  | Mean rewards:  241.71333333333334  | Mean of top 5:  249.0\n",
      "Top  5  scores [ 3  0 13 14  2]\n",
      "Rewards for top:  [250.0, 250.0, 249.5, 248.4, 247.1]\n",
      "Score for elite i  3  is  250.0\n",
      "Score for elite i  0  is  250.0\n",
      "Score for elite i  13  is  250.0\n",
      "Score for elite i  14  is  250.0\n",
      "Score for elite i  2  is  211.2\n",
      "Score for elite i  14  is  250.0\n",
      "Elite selected with index  3  and score 250.0\n",
      "\n",
      "\n",
      "Generation  3  | Mean rewards:  247.56  | Mean of top 5:  250.0\n",
      "Top  5  scores [14 12 11 10  8]\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "Score for elite i  14  is  250.0\n",
      "Score for elite i  12  is  229.8\n",
      "Score for elite i  11  is  231.2\n",
      "Score for elite i  10  is  250.0\n",
      "Score for elite i  8  is  245.0\n",
      "Score for elite i  14  is  250.0\n",
      "Elite selected with index  14  and score 250.0\n"
     ]
    }
   ],
   "source": [
    "for successions in range(numSuccessions):\n",
    "    \n",
    "    folderName = \"Succession\" + str(successions)\n",
    "    if not(os.path.isdir(folderName)):\n",
    "        os.mkdir(folderName)\n",
    "    \n",
    "    for episodes in range(numEpisodes):\n",
    "        trainDQNmodel()\n",
    "        fileName   = \"Episode\" + str(episodes) + \".pth\"\n",
    "        torch.save(policy.state_dict(), \"./\" + folderName + \"/\" + fileName)\n",
    "        \n",
    "    trainEVOModel(folderName, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
