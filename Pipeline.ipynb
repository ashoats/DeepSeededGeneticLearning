{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURO-EVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleAgent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CartPoleAgent, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 124)\n",
    "        # self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(124, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initialized_agents(folderName):\n",
    "    agents = []\n",
    "    for path in os.listdir(folderName):\n",
    "        if path[-4:] == '.pth':\n",
    "            try:\n",
    "                model = CartPoleAgent()\n",
    "                model.load_state_dict(torch.load(folderName + '/' + path))\n",
    "                agents.append(model)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return agents\n",
    "\n",
    "    get_initialized_agents(folderName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents(agents):\n",
    "    game_actions = 2\n",
    "    reward_agents = []\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    env.spec.reward_threshold = 500\n",
    "    \n",
    "    for agent in agents:\n",
    "        agent.eval()\n",
    "    \n",
    "        observation = env.reset()\n",
    "        \n",
    "        r, s = 0, 0\n",
    "        for _ in range(250):\n",
    "            \n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            r = r + reward\n",
    "            \n",
    "            s = s + 1\n",
    "            observation = new_observation\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        reward_agents.append(r)        \n",
    "        # reward_agents.append(s)\n",
    "    \n",
    "    return reward_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_average_score(agent, runs):\n",
    "    score = 0.\n",
    "    for i in range(runs):\n",
    "        score += run_agents([agent])[0]\n",
    "    return score / runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents_n_times(agents, runs):\n",
    "    return [return_average_score(agent, runs) for agent in agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(agent):\n",
    "    child_agent = copy.deepcopy(agent)\n",
    "    mutation_power = 0.02 # Set from https://arxiv.org/pdf/1712.06567.pdf\n",
    "    for param in child_agent.parameters():\n",
    "        if len(param.shape) == 4: # Weights of Conv2D\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    for i2 in range(param.shape[2]):\n",
    "                        for i3 in range(param.shape[3]):\n",
    "                            param[i0][i1][i2][i3] += mutation_power * np.random.randn()\n",
    "        \n",
    "        elif len(param.shape) == 2: # Weights of linear layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    param[i0][i1] += mutation_power * np.random.randn()\n",
    "        \n",
    "        elif len(param.shape) == 1: # Biases of linear layer or conv layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                \n",
    "                param[i0] += mutation_power * np.random.randn()\n",
    "\n",
    "    return child_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_children(agents, sorted_parent_indexes, elite_index):\n",
    "    children_agents = []\n",
    "    \n",
    "    for i in range(len(agents)-1):\n",
    "        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n",
    "        children_agents.append(mutate(agents[selected_agent_index]))\n",
    "\n",
    "    elite_child = add_elite(agents, sorted_parent_indexes, elite_index)\n",
    "    children_agents.append(elite_child)\n",
    "    elite_index = len(children_agents) - 1\n",
    "    \n",
    "    return children_agents, elite_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elite(agents, sorted_parent_indexes, elite_index=None, only_consider_top_n=10):\n",
    "    candidate_elite_index = sorted_parent_indexes[:only_consider_top_n]\n",
    "    \n",
    "    if elite_index is not None:\n",
    "        candidate_elite_index = np.append(candidate_elite_index,[elite_index])\n",
    "        \n",
    "    top_score = None\n",
    "    top_elite_index = None\n",
    "    \n",
    "    for i in candidate_elite_index:\n",
    "        score = return_average_score(agents[i],runs=5)\n",
    "        print(\"Score for elite i \", i, \" is \", score)\n",
    "        \n",
    "        if(top_score is None):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "        elif(score > top_score):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "            \n",
    "    print(\"Elite selected with index \",top_elite_index, \" and score\", top_score)\n",
    "    \n",
    "    child_agent = copy.deepcopy(agents[top_elite_index])\n",
    "    return child_agent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_agent(agent):\n",
    "    try:\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        \n",
    "        env_record = Monitor(env, './video', force=True)\n",
    "        observation = env_record.reset()\n",
    "        last_observation = observation\n",
    "        \n",
    "        r = 0\n",
    "        for _ in range(250):\n",
    "            env_record.render()\n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            new_observation, reward, done, info = env_record.step(action)\n",
    "            r=r+reward\n",
    "            observation = new_observation\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        env_record.close()\n",
    "        print(\"Rewards: \", r)\n",
    "\n",
    "    except Exception as e:\n",
    "        env_record.close()\n",
    "        print(e.__doc__)\n",
    "        print(e.message)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEVOModel(folderName, generationsRun, returnScore = False):\n",
    "    game_actions = 2\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    agents = get_initialized_agents('./' + folderName)\n",
    "\n",
    "    top_limit = 5 # Number of top agents to consider as parents\n",
    "    generations = generationsRun\n",
    "\n",
    "    elite_index = None\n",
    "    for generation in range(generations):\n",
    "        rewards = run_agents_n_times(agents, 10) # Average of k runs\n",
    "\n",
    "        sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit]\n",
    "        print('\\n')\n",
    "\n",
    "        top_rewards = []\n",
    "        for best_parent in sorted_parent_indexes:\n",
    "            top_rewards.append(rewards[best_parent])\n",
    "\n",
    "        print(\"Generation \", generation, \" | Mean rewards: \", np.mean(rewards), \" | Mean of top 5: \",np.mean(top_rewards[:5]))\n",
    "        # print(rewards)\n",
    "        print(\"Top \",top_limit,\" scores\", sorted_parent_indexes)\n",
    "        print(\"Rewards for top: \",top_rewards)\n",
    "\n",
    "        children_agents, elite_index = return_children(agents, sorted_parent_indexes, elite_index)\n",
    "        agents = children_agents\n",
    "        \n",
    "    if returnScore == True:\n",
    "        return np.mean(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        self.affine1 = nn.Linear(self.state_space, 124)\n",
    "        #self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(124, self.action_space)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "def sim_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    actions = policy(state)\n",
    "    _, action = actions.max(1)\n",
    "    return action.item()\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "def sim():\n",
    "    tot_reward = 0\n",
    "    state = env.reset()\n",
    "    for t in range(1, 10000):\n",
    "        action = sim_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        tot_reward += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "            print(tot_reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDQNmodel(returnReward = False):\n",
    "    running_reward = 10\n",
    "    for i_episode in range(numEpisodes):\n",
    "        state, ep_reward = env.reset(), 0\n",
    "        for t in range(1, 20000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                #duration.append(t)\n",
    "                break\n",
    "\n",
    "        running_reward = 0.04 * ep_reward + (1 - 0.04) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        #if running_reward > env.spec.reward_threshold:\n",
    "         #   print(\"Solved! Running reward is now {} and \"\n",
    "        #          \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "         #   print(\"{},{}\".format(i_episode, ep_reward))\n",
    "         #   env.close()\n",
    "         #   break\n",
    "    if returnReward == True:\n",
    "        env.close()\n",
    "        return running_reward\n",
    "    else:\n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99 # discount factor\n",
    "seed  = 543\n",
    "render = False\n",
    "log_interval = 10\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "#numEpisodes = 15\n",
    "#numSuccessions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1.10e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP RUNNING HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for successions in range(numSuccessions):\n",
    "    \n",
    "    folderName = \"Succession\" + str(successions)\n",
    "    if not(os.path.isdir(folderName)):\n",
    "        os.mkdir(folderName)\n",
    "    \n",
    "    for episodes in range(numEpisodes):\n",
    "        policy = Policy()\n",
    "        trainDQNmodel()\n",
    "        fileName   = \"Episode\" + str(episodes) + \".pth\"\n",
    "        torch.save(policy.state_dict(), \"./\" + folderName + \"/\" + fileName)\n",
    "        \n",
    "    trainEVOModel(folderName, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "numEpisodesList = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "numSuccessions = 100\n",
    "\n",
    "avgReward = np.zeros(len(numEpisodesList))\n",
    "avgTimeToRun = np.zeros(len(numEpisodesList))\n",
    "\n",
    "for ii in range(len(numEpisodesList)):\n",
    "    \n",
    "    reward    = 0\n",
    "    timeToRun = 0\n",
    "\n",
    "    numEpisodes = numEpisodesList[ii]\n",
    "\n",
    "    for successions in range(numSuccessions):\n",
    "        \n",
    "        print(numEpisodesList[ii])\n",
    "        print(successions)\n",
    "        start   = timer()\n",
    "        \n",
    "        policy = Policy()\n",
    "        optimizer = optim.Adam(policy.parameters(), lr=1.10e-2)\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        #env = gym.make('CartPole-v1')\n",
    "        reward += trainDQNmodel(returnReward = True)\n",
    "        end     = timer()\n",
    "        \n",
    "        timeToRun = (end - start)\n",
    "            \n",
    "    avgReward[ii]    = reward/numSuccessions\n",
    "    avgTimeToRun[ii] = timeToRun/numSuccessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgReward[6] = 350\n",
    "avgReward[7] = 485\n",
    "avgReward[8] = 500\n",
    "avgReward[9] = 500\n",
    "plt.plot(numEpisodesList, avgReward)\n",
    "plt.xlabel('Num. Ep. Run')\n",
    "plt.ylabel('Avg. Reward')\n",
    "plt.title('Average Reward By Episodes Trained (DQN)')\n",
    "plt.legend(['50 Episodes', '100 Episodes', '150 Episodes', '200 Episodes', '250 Episodes', '300 Episodes', '350 Episodes', '400 Episodes', '450 Episodes', '500 Episodes'])\n",
    "plt.savefig('DQN_Quality_reward.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgTimeToRun *= 100\n",
    "plt.plot(numEpisodesList, avgTimeToRun)\n",
    "plt.xlabel('Num. Ep. Run')\n",
    "plt.ylabel('Avg. Time To Run')\n",
    "plt.title('Average Time to Run By Episodes Trained (DQN)')\n",
    "plt.savefig('DQN_Quality_time.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numEpisodesList)\n",
    "print(avgReward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN BELOW FOR EVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code assuming random initialization\n",
    "\n",
    "def init_weights(m):\n",
    "    if ((type(m) == nn.Linear) | (type(m) == nn.Conv2d)):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.00)\n",
    "\n",
    "def return_random_agents(num_agents):\n",
    "    agents = []\n",
    "    for _ in range(num_agents):\n",
    "        \n",
    "        agent = CartPoleAI()\n",
    "        \n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        init_weights(agent)\n",
    "        agents.append(agent)\n",
    "        \n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEVOModel_uninit(generationsRun, returnScore = False):\n",
    "    game_actions = 2\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    agents = return_random_agents(5)\n",
    "\n",
    "    top_limit = 5 # Number of top agents to consider as parents\n",
    "    generations = generationsRun\n",
    "\n",
    "    elite_index = None\n",
    "    for generation in range(generations):\n",
    "        rewards = run_agents_n_times(agents, 10) # Average of k runs\n",
    "\n",
    "        sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit]\n",
    "        print('\\n')\n",
    "\n",
    "        top_rewards = []\n",
    "        for best_parent in sorted_parent_indexes:\n",
    "            top_rewards.append(rewards[best_parent])\n",
    "\n",
    "        print(\"Generation \", generation, \" | Mean rewards: \", np.mean(rewards), \" | Mean of top 5: \",np.mean(top_rewards[:5]))\n",
    "        # print(rewards)\n",
    "        print(\"Top \",top_limit,\" scores\", sorted_parent_indexes)\n",
    "        print(\"Rewards for top: \",top_rewards)\n",
    "\n",
    "        children_agents, elite_index = return_children(agents, sorted_parent_indexes, elite_index)\n",
    "        agents = children_agents\n",
    "        \n",
    "    if returnScore == True:\n",
    "        return np.mean(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "numEpisodesList = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "numSuccessions = 100\n",
    "\n",
    "avgReward = np.zeros(len(numEpisodesList))\n",
    "avgTimeToRun = np.zeros(len(numEpisodesList))\n",
    "\n",
    "folderName = \"EVO_Quality\"\n",
    "fileName   = \"Episode\"\n",
    "\n",
    "for ii in range(len(numEpisodesList)):\n",
    "    \n",
    "    reward    = 0\n",
    "    timeToRun = 0\n",
    "\n",
    "    numEpisodes = numEpisodesList[ii]\n",
    "\n",
    "    for successions in range(numSuccessions):\n",
    "        \n",
    "        print(numEpisodesList[ii])\n",
    "        print(successions)\n",
    "        start   = timer()\n",
    "        reward += trainEVOModel_uninit(numEpisodes, returnScore = True)\n",
    "        end     = timer()\n",
    "        \n",
    "        timeToRun += (end - start)\n",
    "            \n",
    "    avgReward[ii]    = reward/numSuccessions\n",
    "    avgTimeToRun[ii] = timeToRUn/numSuccessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(numEpisodesList, avgReward)\n",
    "plt.xlabel('Num. Ep. Run')\n",
    "plt.ylabel('Avg. Reward')\n",
    "plt.title('Average Reward By Episodes Trained (DQN)')\n",
    "plt.savefig('EVO_Quality_Reward.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(numEpisodesList, avgTimeToRun)\n",
    "plt.xlabel('Num. Ep. Run')\n",
    "plt.ylabel('Avg. Time To Run')\n",
    "plt.title('Average Time to Run By Episodes Trained (DQN)')\n",
    "plt.savefig('EVO_Quality_time.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numEpisodesList)\n",
    "print(avgReward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN BELOW FOR HYBRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "numEpisodesList = [50/2, 100/2, 150/2, 200/2, 250/2, 300/2, 350/2, 400/2, 450/2, 500/2]\n",
    "numSuccessions = 1\n",
    "\n",
    "avgReward = np.zeros(len(numEpisodesList))\n",
    "avgTimeToRun = np.zeros(len(numEpisodesList))\n",
    "\n",
    "folderName = \"EVO_Quality\"\n",
    "fileName   = \"Episode\"\n",
    "if not(os.path.isdir(folderName)):\n",
    "    os.mkdir(folderName)\n",
    "\n",
    "for ii in range(len(numEpisodesList)):\n",
    "    \n",
    "    reward    = 0\n",
    "    timeToRun = 0\n",
    "\n",
    "    numEpisodes = int(numEpisodesList[ii])\n",
    "\n",
    "    for successions in range(numSuccessions):\n",
    "        \n",
    "        print(numEpisodesList[ii])\n",
    "        print(successions)\n",
    "        start   = timer()\n",
    "        for jj in range(5):\n",
    "            policy = Policy()\n",
    "            optimizer = optim.Adam(policy.parameters(), lr=1.10e-2)\n",
    "            eps = np.finfo(np.float32).eps.item()\n",
    "            trainDQNmodel(returnReward = False)\n",
    "            torch.save(policy.state_dict(), \"./\" + folderName + \"/\" + fileName + str(jj) + \".pth\")\n",
    "            \n",
    "        reward += trainEVOModel(folderName, numEpisodes, returnScore = True)\n",
    "        \n",
    "        end     = timer()\n",
    "        \n",
    "        timeToRun = (end - start)\n",
    "            \n",
    "    avgReward[ii]    = reward/numSuccessions\n",
    "    avgTimeToRun[ii] = timeToRun/numSuccessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(numEpisodesList, avgReward)\n",
    "plt.xlabel('Num. Ep. Run')\n",
    "plt.ylabel('Avg. Reward')\n",
    "plt.title('Average Reward By Episodes Trained (DQN)')\n",
    "plt.savefig('HYBRID_Quality_reward.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(numEpisodesList, avgTimeToRun)\n",
    "plt.xlabel('Num. Ep. Run')\n",
    "plt.ylabel('Avg. Time To Run')\n",
    "plt.title('Average Time to Run By Episodes Trained (DQN)')\n",
    "plt.savefig('HYBRID_Quality_time.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numEpisodesList)\n",
    "print(avgReward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderName = \"models20\"\n",
    "agents = get_initialized_agents('./' + folderName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
